<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>McAteer Portfolio</title>
	<link rel="stylesheet" href="fontawesome/css/all.min.css"> <!-- https://fontawesome.com/ -->
	<link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro&display=swap" rel="stylesheet">
	<!-- https://fonts.google.com/ -->
	<link rel="stylesheet" href="css/bootstrap.min.css">
	<link rel="stylesheet" href="css/templatemo-video-catalog.css">
</head>
<!--
TemplateMo 552 Video Catalog
https://templatemo.com/tm-552-video-catalog
-->
<body>
<div class="tm-page-wrap mx-auto">
	<div class="position-relative">
		<div class="potition-absolute tm-site-header">
			<div class="container-fluid position-relative">
				<div class="row">
					<div class="col-5 col-md-8 ml-auto mr-0">
						<div class="tm-site-nav">
							<nav class="navbar navbar-expand-lg mr-0 ml-auto" id="tm-main-nav">
								<button class="navbar-toggler tm-bg-black py-2 px-3 mr-0 ml-auto collapsed" type="button"
										data-toggle="collapse" data-target="#navbar-nav" aria-controls="navbar-nav"
										aria-expanded="false" aria-label="Toggle navigation">
                                        <span>
                                            <i class="fas fa-bars tm-menu-closed-icon"></i>
                                            <i class="fas fa-times tm-menu-opened-icon"></i>
                                        </span>
								</button>
								<div class="collapse navbar-collapse tm-nav" id="navbar-nav">
									<ul class="navbar-nav text-uppercase">
										<li class="nav-item">
											<a class="nav-link tm-nav-link" href="index.html#content">About</a>
										</li>
										<li class="nav-item">
											<a class="nav-link tm-nav-link" href="index.html#projects">Projects</a>
										</li>
										<li class="nav-item">
											<a class="nav-link tm-nav-link" href="MarkMcAteerResume.pdf" target="_blank">Resume</a>
										</li>
									</ul>
								</div>
							</nav>
						</div>
					</div>
				</div>
			</div>
		</div>
		<div class="tm-welcome-container tm-fixed-header tm-fixed-header-7">
		</div>

		<!-- Header image -->
		<div id="tm-fixed-header-bg"></div>
	</div>

	<!-- Page content -->
	<div class="container-fluid">
		<div class="mx-auto tm-content-container">
			<main>
				<div class="row mb-5 pb-5">
					<div class="col-12">
						<!-- Video description
                        <div class="tm-video-description-box"> -->
						<h1 class="mb-5 tm-video-title"> Berkeley AI: Pacman  </h1>

						<h2 class="tm-text-primary mb-3 tm-catalog-item-title"> Introduction </h2>
						<p class="mb-4"> Archipelago is a fast paced movement roguelike set on a cluster of floating islands. It was created
						in a team of 5 where my role was working on combat programming, enemy AI, and game mechanics such as inventory and health systems. </p>

						<h2 class="tm-text-primary mb-3 tm-catalog-item-title"> Combat </h2>
						<div class="row mb-5 pb-4">
							<div class="col-12">
								<!-- Video player 1422x800 -->
								<video width="854" height="480" controls autoplay>
									<source src="video/Arch_Combat1.mp4" type="video/mp4">
									Your browser does not support the video tag.
								</video>
							</div>
						</div>
						<h5> Overview </h5>
						<p class="mb-4"> The first step I took in creating the melee combat system was an making Input Action for the sword swing that activates the attack animation
						montage. Then I created an IsAttacking variable to ensure the player cannot keep resetting the attack animation once the montage has finished. 
						</p>

						<p class="mb-4"> The next part of combat was to add a combo attack so I created an attack count variable to check which step of the combo attack the 
							player is at. If the player stops attacking the attack count would reset to restart the combo on the next attack step. 
						</p>

						<p class="mb-4"> Then I added anim notifys to each animation montage at the point at which the player has finished each part of a combo attack and can move to 
							the next combo montage animation. 
						</p>

						<p class="mb-4"> The final part of the first step in the combat system was to add an OnSphereOverlap tracking the ik_hand_r bone which destroys 
							a set actor variable on overlap.
						</p>

						<h5> Algorithm Descriptions </h5>
						<p class="mb-4"> The searches all follow the well known defintions of themselves. The depth search first expands the farthest node to the
							left and then goes up the tree. Breadth First Search first expands the closest nodes to the root and then starts expanding recursively
							down on the left most node. Uniform Cost Search is similar to the previous two searches descibed but now uses a priority queue. The prioirty
							of the queue is determined by which ever node has the minimum distance or cost. Finally, A* search will now consider the cost of reaching a node
							as well as the distance to the goal. The A* search function accepts a predefined heuristic function making it an informed search function.
						</p>

						<h2 class="tm-text-primary mb-3 tm-catalog-item-title"> Part 2 </h2>
						<div class="row mb-5 pb-4">
							<div class="col-12">
								<!-- Video player 1422x800 -->
								<video width="1422" height="800" controls autoplay>
									<source src="video/P2.mp4" type="video/mp4">
									Your browser does not support the video tag.
								</video>
							</div>
						</div>
						<h5> Overview </h5>
						<p class="mb-4"> Part Two of the assingment consists of writing a minimax search agent, a minimax search agent using alpha beta pruning,
							and an expectimax agent. </p>

						<h5> Algorithm Descriptions </h5>
						<p class="mb-4"> The minimax search agent will recursively evaluate possible outcomes of different moves asuming that the agent
							is playing optimally. At each level of the tree, the maximizing player chooses the move with the highest score, while the minimzing
							player chooses the move with the lowest score. When alpha beta pruning is added, the agent can disgregard parts of the tree in scenarios
							of which there will be no better option in the child nodes. That part of the tree can then be removed, or pruned, which will improve the overall
							efficeny of the search.  Expectimax is an extension of of the minimax algorithm where a chance node represents
							the different possible outcomes of a probablistic event. The algorithm calculates the expected value at chance nodes by takign the average
							of the possible outcomes, weighted by their probabliites.
						</p>

						<h2 class="tm-text-primary mb-3 tm-catalog-item-title"> Part 3 </h2>
						<div class="row mb-5 pb-4">
							<div class="col-12">
								<!-- Video player 1422x800 -->
								<video width="1422" height="800" controls autoplay>
									<source src="video/P3.mp4" type="video/mp4">
									Your browser does not support the video tag.
								</video>
							</div>
						</div>
						<h5> Overview </h5>
						<p class="mb-4"> Part 3 of the project consisted of writing a value iteration function and a q-learning agent on a
							pacman grid. In Part 1 and Part 2, the pacman agent would go where it was told. Now, the agent has an 80% change of
							going the way it is told and 10% chances of going left and right.   </p>

						<h5> Value Iteration </h5>
						<p class="mb-4"> The value iteration function taken in a Markov Decision Process (MDP) on initialization, and runs a
							value iteration for a given number of iterations using a supplied discount factor. The value iteration function will supply
							an optimal policy, or a solution that states waht an agetn must do for any state the agent might reach. The most important function
							to complete the value iteration function in the getQValue() function. It will return the q value of a state action pair which is a
							utility value or the expected utility of taking a given action in a given state.
						</p>
						<p class="mb-4"> To compute the Q-Value use the equation: ‚àë P(s'|s, a)[R(s, a, s') + ùõæU[s'] where: </p>
						<p class="mb-4"> P(s'|s, a) is the probability of reaching state s' if an action, a, is done in state s.</p>
						<p class="mb-4"> R(s, a, s') is the reward the agent recieves for every transition from s to s' for action, a. </p>
						<p class="mb-4"> ùõæU[s'] is the  the discount factor multiplied by the value/utility of the transition state.</p>
						<h5> Q-Learning Agent </h5>
						<p class="mb-4"> The algorithm maintains a Q-value for each state-action pair, representing the expected cumulative reward
							of taking a specific action in a given state and following the optimal policy thereafter. Through exploration of the
							environment, the agent updates its Q-values based on the observed rewards and refines its policy over time.
						</p>
						<p class="mb-4"> To compute the Q-Value in the update function use the equation: Q(s, a) <- (1 - Œ±)Q(s, a) + (Œ±)[R(s, a, s') + ùõæmaxQ(s', a')] where: </p>
						<p class="mb-4"> ùõæ is the discount rate</p>
						<p class="mb-4"> Œ± is the alpha </p>
						<p class="mb-4"> s is the current state</p>
						<p class="mb-4"> s' is the next state</p>
						<p class="mb-4"> a is the current action</p>
					</div>
				</div>
			</main>
		</div> <!-- .tm-content-container -->
	</div>
</div>

<script src="js/jquery-3.4.1.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script>
	$(document).ready(function() {
		$('.tm-likes-box').click(function(e) {
			e.preventDefault();
			$(this).toggleClass('tm-liked');

			if($(this).hasClass('tm-liked')) {
				$('#tm-likes-count').html('486 likes');
			} else {
				$('#tm-likes-count').html('485 likes');
			}
		});
	});
</script>
</body>
</html>
